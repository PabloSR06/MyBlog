Recently I have been working on a project using a vision ml model, specifically I have been training [YOLOv8](https://github.com/ultralytics/ultralytics). After training some images I came across with various graphs and values, each representing different parameters and metrics. These parameters are critical in evaluating the performance of the model and understanding how well it is learning from the data.

The Key Metric Score to review are:
- Precision
- Recall
- Confidence
- F1 Score


Let's set an example to better undestanting: we are working on a machine learning model to detect whether an email is spam or not. In total we have 1000 emails.

- **True Positives (TP)**: 500 (The email marked as spam is spam)
- **False Positives (FP)**: 150 (The email marked as spam is not spam)
- **True Negatives (TN)**: 300 (The email marked as  not-spam is not-spam)
- **False Negatives (FN)**: 50 (The email marked as not-spam is spam)



### Precision

Precision is a metric that measures the accuracy of positive predictions generated by a model. It is calculated as the ratio of positive cases to the total number of predicted positive cases. A high accuracy value means a minimal incidence of false positives.

This metric is often used in binary classification environments to evaluate the effectiveness of a model in predicting the positive class.


> The formula for precision is: Precision = True Positive / (True Positive + False Positive)
>
> **Precision** = 500 / (500 + 150) = 0.769



### Recall

Recall also referred to as the True Positive Rate (TPR),is used as a performance metric for classification models, is the fraction of positives that are correctly classified.
It measures how many times the model gets it right when it tries to identify examples of a particular class. For example, if our class of interest is "dogs", recall would tell us how well the model is able to find cats in a set of images.


> The recall formula is: Recall = True Positive / (True Positive + False Negative)
>
> **Recall** = 500 / (500+50) = 0.909


### Confidence

It's the value of certainty that the model's have about the prediction, the probability that a predicted outcome by a model is correct. A high confidence does not mean that the prediction is correct. You can get high confidence in a wrong answer, that is a sign that the model needs better training.

Confidence score is often used if you need to decide between answers or whether to do something after a prediction if the score meets a certain threshold.



### F1 Score

The F1 score is a metric that evaluates a model's accuracy considering both precision and recall parameters. This score is useful in situations where the data has an uneven class distribution. It essentially provides a balanced measure of a model's performance, particularly in situations where either a false positive or a false negative could have significant consequences. 


> F1 score combines both of these precision and recall and symmetrically represents them in the formula: F1 = 2 * (precision * recall) / (precision + recall)
>
> **F1** = 2 * ( (0.769 * 0.909) / (0.769 + 0.909)) = 0.8331 